{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaypatil588/groundedSAM/blob/main/grounded_sam_colab_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!which nvcc\n",
        "!nvcc --version"
      ],
      "metadata": {
        "id": "Jco0PzCGvAkx",
        "outputId": "62fcee5f-2b8e-49c7-a623-d8443c6a3dda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda/bin/nvcc\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Grounded SAM Inpainting Demo](https://github.com/IDEA-Research/Grounded-Segment-Anything/raw/main/assets/grounded_sam_inpainting_demo.png)\n",
        "\n",
        "## Why this project?\n",
        "\n",
        "- [Segment Anything](https://github.com/facebookresearch/segment-anything) is a strong segmentation model. But it need prompts (like boxes/points) to generate masks.\n",
        "- [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) is a strong zero-shot detector which enable to generate high quality boxes and labels with free-form text.\n",
        "- The combination of the two models enable **to detect and segment everything** with text inputs!\n",
        "\n"
      ],
      "metadata": {
        "id": "O49XQD8rwNUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install"
      ],
      "metadata": {
        "id": "kAi4IkEfKi5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export CUDA_HOME=/usr/local/cuda\n",
        "export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH\n",
        "echo \"Using CUDA version:\" $(nvcc --version | grep release)\n",
        "\n",
        "apt-get update -qq\n",
        "apt-get install -y -qq build-essential swig libsm6 libxrender1 libxext6\n",
        "\n",
        "pip install --quiet torch==2.1.0+cu118 torchvision==0.16.0+cu118 \\\n",
        "    --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "cd /content/Grounded-Segment-Anything\n",
        "pip install -r requirements.txt || echo \"Requirements install failed\"\n",
        "\n",
        "cd GroundingDINO\n",
        "pip install --verbose . || echo \"groundingdino build failed\"\n"
      ],
      "metadata": {
        "id": "BaXjn65ZvmaS",
        "outputId": "409f1701-3c24-4c7f-e3bb-b71a57b1cfe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA version: Cuda compilation tools, release 12.5, V12.5.82\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 GB 374.4 kB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 107.2 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.2/89.2 MB 9.8 MB/s eta 0:00:00\n",
            "Requirements install failed\n",
            "Using pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n",
            "groundingdino build failed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.0+cu118 which is incompatible.\n",
            "bash: line 11: cd: /content/Grounded-Segment-Anything: No such file or directory\n",
            "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n",
            "bash: line 14: cd: GroundingDINO: No such file or directory\n",
            "ERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSJqZHr3JTSB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c617f60-71b9-4302-d1a5-f626b6bef2ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Grounded-Segment-Anything'...\n",
            "remote: Enumerating objects: 1807, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 1807 (delta 9), reused 1 (delta 1), pack-reused 1789 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1807/1807), 155.84 MiB | 15.49 MiB/s, done.\n",
            "Resolving deltas: 100% (830/830), done.\n",
            "Updating files: 100% (247/247), done.\n",
            "/content/Grounded-Segment-Anything\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "\n",
        "!git clone https://github.com/IDEA-Research/Grounded-Segment-Anything\n",
        "\n",
        "%cd /content/Grounded-Segment-Anything\n",
        "!pip install -q -r requirements.txt\n",
        "%cd /content/Grounded-Segment-Anything/GroundingDINO\n",
        "!pip install -q .\n",
        "%cd /content/Grounded-Segment-Anything/segment_anything\n",
        "!pip install -q .\n",
        "%cd /content/Grounded-Segment-Anything"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "MRkQUrtjKpbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "\n",
        "sys.path.append(os.path.join(os.getcwd(), \"GroundingDINO\"))\n",
        "\n",
        "import argparse\n",
        "import copy\n",
        "\n",
        "from IPython.display import display\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from torchvision.ops import box_convert\n",
        "\n",
        "# Grounding DINO\n",
        "import GroundingDINO.groundingdino.datasets.transforms as T\n",
        "from GroundingDINO.groundingdino.models import build_model\n",
        "from GroundingDINO.groundingdino.util import box_ops\n",
        "from GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
        "from GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
        "from GroundingDINO.groundingdino.util.inference import annotate, load_image, predict\n",
        "\n",
        "import supervision as sv\n",
        "\n",
        "# segment anything\n",
        "from segment_anything import build_sam, SamPredictor\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# diffusers\n",
        "import PIL\n",
        "import requests\n",
        "import torch\n",
        "from io import BytesIO\n",
        "from diffusers import StableDiffusionInpaintPipeline\n",
        "\n",
        "\n",
        "from huggingface_hub import hf_hub_download"
      ],
      "metadata": {
        "id": "6Osu6KERJeTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load models"
      ],
      "metadata": {
        "id": "IFnZ_nwcKuUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "004CGse3NRS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grounding DINO model"
      ],
      "metadata": {
        "id": "qzhOnA6ALQlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu'):\n",
        "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
        "\n",
        "    args = SLConfig.fromfile(cache_config_file)\n",
        "    args.device = device\n",
        "    model = build_model(args)\n",
        "\n",
        "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "    checkpoint = torch.load(cache_file, map_location=device)\n",
        "    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
        "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
        "    _ = model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "id": "dPTgK04-J1U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
        "ckpt_filenmae = \"groundingdino_swinb_cogcoor.pth\"\n",
        "ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n",
        "\n",
        "\n",
        "groundingdino_model = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename, device)"
      ],
      "metadata": {
        "id": "73fWWbE0KBZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SAM"
      ],
      "metadata": {
        "id": "uRsDao4wLV4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "sam_checkpoint = 'sam_vit_h_4b8939.pth'\n",
        "\n",
        "sam_predictor = SamPredictor(build_sam(checkpoint=sam_checkpoint).to(device))"
      ],
      "metadata": {
        "id": "QNyrl-WMLVQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stable Diffusion (Inpainting)"
      ],
      "metadata": {
        "id": "ZiDBwfq0LeVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sd_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-2-inpainting\",\n",
        "    torch_dtype=torch.float16,\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "KmSGdNxYLkGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "y86GkczAOAVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load image\n",
        "def download_image(url, image_file_path):\n",
        "    r = requests.get(url, timeout=4.0)\n",
        "    if r.status_code != requests.codes.ok:\n",
        "        assert False, 'Status code error: {}.'.format(r.status_code)\n",
        "\n",
        "    with Image.open(BytesIO(r.content)) as im:\n",
        "        im.save(image_file_path)\n",
        "    print('Image downloaded from url: {} and saved to: {}.'.format(url, image_file_path))\n",
        "\n",
        "\n",
        "local_image_path = \"assets/inpaint_demo.jpg\"\n",
        "image_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
        "\n",
        "download_image(image_url, local_image_path)\n",
        "image_source, image = load_image(local_image_path)\n",
        "Image.fromarray(image_source)"
      ],
      "metadata": {
        "id": "dad-C9F_OWn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grounding DINO for detection"
      ],
      "metadata": {
        "id": "NaEIyVwjOCam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# detect object using grounding DINO\n",
        "def detect(image, text_prompt, model, box_threshold = 0.3, text_threshold = 0.25):\n",
        "  boxes, logits, phrases = predict(\n",
        "      model=model,\n",
        "      image=image,\n",
        "      caption=text_prompt,\n",
        "      box_threshold=box_threshold,\n",
        "      text_threshold=text_threshold\n",
        "  )\n",
        "\n",
        "  annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
        "  annotated_frame = annotated_frame[...,::-1] # BGR to RGB\n",
        "  return annotated_frame, boxes"
      ],
      "metadata": {
        "id": "lm8fGM5XOkqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_frame, detected_boxes = detect(image, text_prompt=\"bench\", model=groundingdino_model)\n",
        "Image.fromarray(annotated_frame)"
      ],
      "metadata": {
        "id": "YVjdmTWOPxTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAM for segmentation"
      ],
      "metadata": {
        "id": "6p7cHFDKQw5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def segment(image, sam_model, boxes):\n",
        "  sam_model.set_image(image)\n",
        "  H, W, _ = image.shape\n",
        "  boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
        "\n",
        "  transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(device), image.shape[:2])\n",
        "  masks, _, _ = sam_model.predict_torch(\n",
        "      point_coords = None,\n",
        "      point_labels = None,\n",
        "      boxes = transformed_boxes,\n",
        "      multimask_output = False,\n",
        "      )\n",
        "  return masks.cpu()\n",
        "\n",
        "\n",
        "def draw_mask(mask, image, random_color=True):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "\n",
        "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
        "    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
        "\n",
        "    return np.array(Image.alpha_composite(annotated_frame_pil, mask_image_pil))"
      ],
      "metadata": {
        "id": "k8-1lhHjQ2dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segmented_frame_masks = segment(image_source, sam_predictor, boxes=detected_boxes)\n",
        "annotated_frame_with_mask = draw_mask(segmented_frame_masks[0][0], annotated_frame)\n",
        "Image.fromarray(annotated_frame_with_mask)"
      ],
      "metadata": {
        "id": "puUUEAAjSlug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stable Diffusion for inpainting"
      ],
      "metadata": {
        "id": "Ct786WclTVdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create mask images\n",
        "mask = segmented_frame_masks[0][0].cpu().numpy()\n",
        "inverted_mask = ((1 - mask) * 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "image_source_pil = Image.fromarray(image_source)\n",
        "image_mask_pil = Image.fromarray(mask)\n",
        "inverted_image_mask_pil = Image.fromarray(inverted_mask)\n",
        "\n",
        "\n",
        "display(*[image_source_pil, image_mask_pil, inverted_image_mask_pil])"
      ],
      "metadata": {
        "id": "wmBncdm8TVF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image(image, mask, prompt, negative_prompt, pipe, seed):\n",
        "  # resize for inpainting\n",
        "  w, h = image.size\n",
        "  in_image = image.resize((512, 512))\n",
        "  in_mask = mask.resize((512, 512))\n",
        "\n",
        "  generator = torch.Generator(device).manual_seed(seed)\n",
        "\n",
        "  result = pipe(image=in_image, mask_image=in_mask, prompt=prompt, negative_prompt=negative_prompt, generator=generator)\n",
        "  result = result.images[0]\n",
        "\n",
        "  return result.resize((w, h))"
      ],
      "metadata": {
        "id": "VWKOb_ZWWx2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"A sofa, high quality, detailed, cyberpunk, futuristic, with a lot of details, and a lot of colors.\"\n",
        "negative_prompt=\"low resolution, ugly\"\n",
        "seed = 32 # for reproducibility\n",
        "\n",
        "generated_image = generate_image(image=image_source_pil, mask=image_mask_pil, prompt=prompt, negative_prompt=negative_prompt, pipe=sd_pipe, seed=seed)\n",
        "generated_image"
      ],
      "metadata": {
        "id": "IkRMVCOkYU68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"a beach with turquoise water, sand, and coconuts\"\n",
        "negative_prompt=\"people, low resolution, ugly\"\n",
        "seed = 32 # for reproducibility\n",
        "\n",
        "generated_image = generate_image(image_source_pil, inverted_image_mask_pil, prompt, negative_prompt, sd_pipe, seed)\n",
        "generated_image"
      ],
      "metadata": {
        "id": "s3LxZK1EY69d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BFPPGHaOuTJy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}